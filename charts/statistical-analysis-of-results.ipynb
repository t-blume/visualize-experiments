{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tabulate\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import re\n",
    "\n",
    "from itertools import cycle, islice\n",
    "\n",
    "# Fitting Linear Regression to the dataset \n",
    "from sklearn.linear_model import LinearRegression \n",
    "# Fitting Polynomial Regression to the dataset \n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_width = 8\n",
    "fig_height = 5\n",
    "\n",
    "params = {\n",
    "    'axes.labelsize': 12, # fontsize for x and y labels (was 10)\n",
    "    'axes.titlesize': 12,\n",
    "    #'text.fontsize': 8, # was 10\n",
    "    'legend.fontsize': 12, # was 10\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'text.usetex': True,\n",
    "    'text.usetex': False,\n",
    "    'figure.figsize': [fig_width,fig_height],\n",
    "    'font.family': 'serif',\n",
    "    'grid.linestyle': '--',\n",
    "    'lines.linewidth': 2\n",
    "}\n",
    "\n",
    "matplotlib.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'aggregated_data'\n",
    "out_dir = 'aggregated_data/stats'\n",
    "\n",
    "\n",
    "appname = 'LUBM'\n",
    "data_size_dir = 'LUBM'\n",
    "collections = ['schemex', 'attribute', 'type']\n",
    "suffix = ''\n",
    "hide_legend = False\n",
    "\n",
    "\"\"\"\n",
    "appname = 'BSBM'\n",
    "data_size_dir = 'BSBM'\n",
    "collections =['schemex', 'attribute', 'type']\n",
    "suffix = ''\n",
    "hide_legend = True\n",
    "\n",
    "appname = 'dyldo_y2019_core'\n",
    "data_size_dir = 'DyLDO-core'\n",
    "collections =['schemex', 'attribute', 'type']\n",
    "suffix = '-test-1_clean'\n",
    "hide_legend = True\n",
    "\n",
    "appname = 'dyldo_y2019_full'\n",
    "data_size_dir = 'DyLDO-ext'\n",
    "collections =['schemex', 'attribute', 'type']\n",
    "suffix = ''\n",
    "hide_legend = True\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = join(data_dir, data_size_dir)\n",
    "onlyfiles = [f for f in listdir(curr_dir) if isfile(join(curr_dir, f)) and f.endswith(\".csv\")]\n",
    "onlyfiles.sort(key=natural_keys)\n",
    "\n",
    "degree_frame = pd.DataFrame()\n",
    "i = 0\n",
    "for filename in onlyfiles:\n",
    "    with open(os.path.join(curr_dir, filename), 'r') as f:\n",
    "        df = pd.read_csv(f, sep=',')\n",
    "        if 'iteration' in filename:\n",
    "            iteration = filename.replace('iteration', '')\n",
    "            iteration = iteration.replace('-', '')\n",
    "            iteration = iteration.replace('degree.csv', '')\n",
    "            iteration = iteration.replace('.gz', '')\n",
    "            iteration = iteration.replace('.nq', '')\n",
    "            iteration = iteration.replace('.nt', '')\n",
    "            i = int(iteration)\n",
    "        else:\n",
    "            i = i + 1\n",
    "\n",
    "        df['Iteration'] = i\n",
    "        df = df.set_index('Iteration')\n",
    "        if degree_frame.empty:\n",
    "            degree_frame = df\n",
    "        else:\n",
    "            degree_frame = degree_frame.append(df)\n",
    "\n",
    "#plot = degree_frame['max_degree'].plot()  \n",
    "if appname == 'dyldo_y2019_core':\n",
    "    degree_frame = degree_frame.drop(degree_frame.index[[21,22]])\n",
    "\n",
    "\n",
    "if appname == 'dyldo_y2019_full':\n",
    "    degree_frame = degree_frame.head()\n",
    "    iterations = 5\n",
    "    ticks = 1.0\n",
    "    print('drop the beat')\n",
    "\n",
    "plot_frame = degree_frame[['avg_degree', 'avg_indegree', 'avg_outdegree']]\n",
    "\n",
    "plot_frame.columns = ['degree', 'in-degree', 'out-degree']\n",
    "\n",
    "boxplot = plot_frame.boxplot() \n",
    "\n",
    "plt.savefig(out_dir + '/' + data_size_dir + '-degree.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(degree_frame['avg_degree'].mean())\n",
    "display(degree_frame['avg_degree'].std())\n",
    "\n",
    "display(degree_frame['avg_indegree'].mean())\n",
    "display(degree_frame['avg_indegree'].std())\n",
    "\n",
    "display(degree_frame['avg_outdegree'].mean())\n",
    "display(degree_frame['avg_outdegree'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main correlation analysis\n",
    "\n",
    "appnames = ['BSBM', 'dyldo_y2019_core', 'dyldo_y2019_full']\n",
    "appnames = ['dyldo_y2019_core']\n",
    "\n",
    "datasets = ['LUBM', 'BSBM', 'DyLDO-core', 'DyLDO-ext']\n",
    "datasets = ['DyLDO-core']\n",
    "\n",
    "models = ['schemex', 'attribute', 'type']\n",
    "models = ['attribute']\n",
    "\n",
    "\n",
    "size_frame = pd.DataFrame(columns=['dataset', 'edges', 'types', 'bytes'])\n",
    "\n",
    "index = 0\n",
    "for dataset in datasets:\n",
    "    curr_dir = join(data_dir, dataset)\n",
    "    onlyfiles = [f for f in listdir(curr_dir) if isfile(join(curr_dir, f)) and f.endswith(\".txt\")]\n",
    "    onlyfiles.sort(key=natural_keys)\n",
    "\n",
    "\n",
    "    temp_frame = pd.DataFrame(columns=['dataset', 'edges', 'types', 'bytes', 'degree'])\n",
    "    i = 0\n",
    "    for filename in onlyfiles:\n",
    "        f = open(join(curr_dir,filename), 'r')\n",
    "        f_degree = open(join(curr_dir,filename.replace(\".txt\", \"-degree.csv\")), 'r')\n",
    "        content = f.read().split('\\n')\n",
    "\n",
    "        df = pd.read_csv(f_degree, sep=',')\n",
    "        df['Iteration'] = i\n",
    "        df = df.set_index('Iteration')\n",
    "\n",
    "        types = int(content[0])\n",
    "        edges = int(content[1])\n",
    "        bytez = int(content[2])\n",
    "        degree = df['avg_degree'].values[0]\n",
    "\n",
    "        if 'iteration' in filename:\n",
    "            iteration = filename.replace('iteration', '')\n",
    "            iteration = iteration.replace('-', '')\n",
    "            iteration = iteration.replace('.txt', '')\n",
    "            iteration = iteration.replace('.gz', '')\n",
    "            iteration = iteration.replace('.nq', '')\n",
    "            iteration = iteration.replace('.nt', '')\n",
    "            i = int(iteration)\n",
    "        else:\n",
    "            i = i + 1\n",
    "        temp_frame.loc[i] = [dataset, edges, types, bytez, degree]\n",
    "        \n",
    "\n",
    "    \n",
    "    for model in models:\n",
    "        f_space = open(os.path.join(data_dir, appnames[index] + '_'+model+'-update-time-and-space.csv'), 'r')\n",
    "        #print(f_space)\n",
    "        sf = pd.read_csv(f_space, sep=',')\n",
    "        sf = sf.set_index('Iteration')\n",
    "        #display(sf)\n",
    "        temp_frame[model+'-summarization-ratio'] = sf['Imprint links'] / sf['Schema Elements (SE)']\n",
    "        temp_frame[model+'VHI'] = sf['Sec. Index Size (bytes)']\n",
    "        temp_frame[model+'-compression'] = temp_frame[model+'VHI'] / temp_frame['bytes']\n",
    "        if 'instances' not in temp_frame:\n",
    "            temp_frame['instances'] = sf['Imprint links']\n",
    "        f_time = open(os.path.join(data_dir, appnames[index] + '_'+model+'-performance.csv'), 'r')\n",
    "        #print(f_time)\n",
    "        tf = pd.read_csv(f_time, sep=',')\n",
    "        tf = tf.set_index('Iteration')\n",
    "        #display(tf)\n",
    "        temp_frame[model+'schema-computation'] = tf['Schema Computation']\n",
    "        temp_frame[model+'update'] = tf['Updates']\n",
    "        temp_frame[model+'incremental'] = tf['Total']\n",
    "        temp_frame[model+'batch'] = tf['Batch']\n",
    "        temp_frame[model+'speed-up'] = tf['Batch'] / tf['Total']\n",
    "\n",
    "        \n",
    "        f_change = open(os.path.join(data_dir, appnames[index] + '_'+model+'-changes.csv'), 'r')\n",
    "        cf = pd.read_csv(f_change, sep=',')\n",
    "        temp_frame[model+':changes'] = cf['ChangedSchemaStructures (SE_mod)'] +  ((sf['Imprint links'] - sf['Imprint links'].shift(1,fill_value=0)).abs())\n",
    "        temp_frame[model+':updates'] = cf['NewlyObservedSchema (SE_new)'] + cf['DeletedSchemaStructures (SE_del)']\n",
    "        temp_frame[model+':change-update-ratio'] = temp_frame[model+':changes']/(temp_frame[model+':updates'] + 1)\n",
    "        #attribute:changes\n",
    "        temp_frame[model+':change-size-ratio'] = temp_frame[model+':changes']/(temp_frame['instances'])\n",
    "        temp_frame[model+':update-size-ratio'] = temp_frame[model+':updates']/(sf['Schema Elements (SE)'])\n",
    "\n",
    "    if dataset == 'DyLDO-core':\n",
    "        temp_frame = temp_frame.drop(temp_frame.index[[20,21]])\n",
    "    if dataset == 'DyLDO-ext':\n",
    "        temp_frame = temp_frame.head(5)\n",
    "        \n",
    "    if size_frame.empty:\n",
    "        size_frame = temp_frame\n",
    "    else:\n",
    "        size_frame = size_frame.append(temp_frame)\n",
    "        \n",
    "  \n",
    "    index = index + 1\n",
    "\n",
    "size_frame.to_csv(out_dir + '/' + 'correlation-stats.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "display(size_frame.head())\n",
    "for model in models:\n",
    "    print(\"Summarization Ratio ({}): \".format(model))\n",
    "    print(\"\\tMean:\\t {}\".format(size_frame[model+'-summarization-ratio'].mean()))\n",
    "    print(\"\\tStd:\\t {}\".format(size_frame[model+'-summarization-ratio'].std()))\n",
    "    print(\"\\tMin:\\t {}\".format(size_frame[model+'-summarization-ratio'].min()))\n",
    "    print(\"\\tMAX:\\t {}\".format(size_frame[model+'-summarization-ratio'].max()))\n",
    "\n",
    "    print(\"Change-Update Ratio ({}): \".format(model))\n",
    "    print(\"\\tMean:\\t {}\".format(size_frame[model+':change-update-ratio'].mean()))\n",
    "    print(\"\\tStd:\\t {}\".format(size_frame[model+':change-update-ratio'].std()))\n",
    "    print(\"\\tMin:\\t {}\".format(size_frame[model+':change-update-ratio'].min()))\n",
    "    print(\"\\tMAX:\\t {}\".format(size_frame[model+':change-update-ratio'].max()))\n",
    "    \n",
    "    print(\"Incremental Time ({}): \".format(model))\n",
    "    print(\"\\tMean:\\t {}\".format(size_frame[model+'incremental'].mean()))\n",
    "    print(\"\\tStd:\\t {}\".format(size_frame[model+'incremental'].std()))\n",
    "    print(\"\\tMin:\\t {}\".format(size_frame[model+'incremental'].min()))\n",
    "    print(\"\\tMAX:\\t {}\".format(size_frame[model+'incremental'].max()))\n",
    "    \n",
    "    print(\"Batch Time ({}): \".format(model))\n",
    "    print(\"\\tMean:\\t {}\".format(size_frame[model+'batch'].mean()))\n",
    "    print(\"\\tStd:\\t {}\".format(size_frame[model+'batch'].std()))\n",
    "    print(\"\\tMin:\\t {}\".format(size_frame[model+'batch'].min()))\n",
    "    print(\"\\tMAX:\\t {}\".format(size_frame[model+'batch'].max()))\n",
    "    \n",
    "    print(\"Speed-up Time ({}): \".format(model))\n",
    "    print(\"\\tMean:\\t {}\".format(size_frame[model+'speed-up'].mean()))\n",
    "    print(\"\\tStd:\\t {}\".format(size_frame[model+'speed-up'].std()))\n",
    "    print(\"\\tMin:\\t {}\".format(size_frame[model+'speed-up'].min()))\n",
    "    print(\"\\tMAX:\\t {}\".format(size_frame[model+'speed-up'].max()))\n",
    "    print(\"-----------\")\n",
    "    \n",
    "# display(size_frame)\n",
    "# print('schemex')\n",
    "# display(size_frame['attribute-compression'])\n",
    "# print(size_frame['schemex-compression'].mean())\n",
    "# print(size_frame['schemex-compression'].std())\n",
    "\n",
    "# print('attribute')\n",
    "# print(size_frame['attribute-compression'].mean())\n",
    "# print(size_frame['attribute-compression'].std())\n",
    "\n",
    "# print('type')\n",
    "# print(size_frame['type-compression'].mean())\n",
    "# print(size_frame['type-compression'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = ['attribute:change-size-ratio'] #'change-ratio'\n",
    "x_values = ['attribute:update-size-ratio'] #'change-ratio'\n",
    "\n",
    "y_value = ['attributespeed-up']\n",
    "\n",
    "\n",
    "#x_values = ['edges'] #'change-ratio'\n",
    "#y_value = ['vertexHashIndex']\n",
    "\n",
    "X = size_frame[x_values].values # , 'change-ratio'\n",
    "y = size_frame[y_value].values\n",
    "\n",
    "#display(X)\n",
    "\n",
    "lin = LinearRegression() \n",
    "a = [v[0] for v in X]\n",
    "#b = [v[1] for v in X]\n",
    "#display(a)\n",
    "lin.fit(X, y) \n",
    "display(lin.coef_)\n",
    "# Visualising the Linear Regression results \n",
    "plt.scatter(a, y, color = 'blue') \n",
    "#plt.scatter(b, y, color = 'green') \n",
    "\n",
    "plt.plot(X, lin.predict(X), color = 'red') \n",
    "plt.title('Linear Regression') \n",
    "plt.xlabel(x_values) \n",
    "plt.ylabel(y_value) \n",
    "\n",
    "plt.show() \n",
    "lin.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Do edges correlate with schema-computation?\")\n",
    "corrtest = pearsonr(size_frame['edges'], size_frame['attributeschema-computation'])  \n",
    "display(corrtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp = size_frame[['edges', 'schemexschema-computation', 'attributeschema-computation', 'typeschema-computation']]\n",
    "# display(tmp.head())\n",
    "# display(tmp.astype('float64').corr())\n",
    "\n",
    "\"\"\"\n",
    "Build a frame for all three summary models:\n",
    "\"\"\"\n",
    "\n",
    "full_frame = pd.DataFrame(columns=['vertices', 'edges', 'vertexHashIndex', 'schema-computation'])\n",
    "full_frame['vertices'] = size_frame['instances'].append(size_frame['instances']).append(size_frame['instances'])\n",
    "full_frame['edges'] = size_frame['edges'].append(size_frame['edges']).append(size_frame['edges'])\n",
    "full_frame['vertexHashIndex'] = size_frame['schemexVHI'].append(size_frame['attributeVHI']).append(size_frame['typeVHI'])\n",
    "full_frame['schema-computation'] = size_frame['schemexschema-computation'].append(size_frame['attributeschema-computation']).append(size_frame['typeschema-computation'])\n",
    "display(full_frame)\n",
    "\n",
    "print(\"Do edges correlate with schema-computation?\")\n",
    "corrtest = pearsonr(full_frame['edges'], full_frame['schema-computation'])  \n",
    "display(corrtest)\n",
    "\n",
    "\n",
    "print(\"Do vertices correlate with vertexHashIndex?\")\n",
    "corrtest = pearsonr(full_frame['vertices'], full_frame['vertexHashIndex'])  \n",
    "display(corrtest)\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Do edges correlate with vertexHashIndex?\")\n",
    "# corrtest = pearsonr(full_frame['edges'], full_frame['vertexHashIndex'])  \n",
    "# display(corrtest)\n",
    "# corrtest = pearsonr(size_frame['edges'], size_frame['attributeschema-computation'])  \n",
    "# display(corrtest)\n",
    "\n",
    "# corrtest = pearsonr(size_frame['edges'], size_frame['typeschema-computation'])  \n",
    "# display(corrtest)\n",
    "\n",
    "display(full_frame['vertexHashIndex'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = ['edges'] #'change-ratio'\n",
    "y_value = ['schema-computation']\n",
    "\n",
    "\n",
    "x_values = ['vertices'] #'change-ratio'\n",
    "y_value = ['vertexHashIndex']\n",
    "\n",
    "#x_values = ['edges'] #'change-ratio'\n",
    "#y_value = ['vertexHashIndex']\n",
    "\n",
    "X = full_frame[x_values].values # , 'change-ratio'\n",
    "y = full_frame[y_value].values\n",
    "\n",
    "#display(X)\n",
    "\n",
    "lin = LinearRegression() \n",
    "a = [v[0] for v in X]\n",
    "#b = [v[1] for v in X]\n",
    "#display(a)\n",
    "lin.fit(X, y) \n",
    "display(lin.coef_)\n",
    "# Visualising the Linear Regression results \n",
    "plt.scatter(a, y, color = 'blue') \n",
    "#plt.scatter(b, y, color = 'green') \n",
    "\n",
    "plt.plot(X, lin.predict(X), color = 'red') \n",
    "plt.title('Linear Regression') \n",
    "plt.xlabel(x_values) \n",
    "plt.ylabel(y_value) \n",
    "\n",
    "plt.show() \n",
    "lin.score(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "change_frame = pd.DataFrame(columns=['SE-ADD', 'SE-DEL','SE-MOD', 'SE-unchanged', \n",
    "                                     'instances', 'stability-ratio', 'change-ratio', \n",
    "                                     'update-ratio'])\n",
    "iterations = 0\n",
    "\n",
    "for collection in collections:\n",
    "    with open(os.path.join(data_dir, appname + '_'+collection+suffix+'-changes.csv'), 'r') as f:\n",
    "        df = pd.read_csv(f, sep=',')\n",
    "        \n",
    "        iterations = max(iterations, len(df))\n",
    "        f2 = open(os.path.join(data_dir, appname + '_'+collection+suffix+'-update-time-and-space.csv'), 'r')\n",
    "        df2 = pd.read_csv(f2, sep=',')\n",
    "        df2.head()\n",
    "        \n",
    "        df2['Imprint links add-del'] = (df2['Imprint links'] - df2['Imprint links'].shift(1,fill_value=0)).abs()\n",
    "\n",
    "        \n",
    "        del df['TotalNumberOfNewInstances'] \n",
    "        del df['InstanceAddedWithKnownSchema (PE_add)']\n",
    "        del df['InstancesDeleted (PE_del)']\n",
    "        del df['ChangedSchemaStructuresBecauseOfNeighbor'] \n",
    "        del df['PayloadEntriesAdded'] \n",
    "        del df['PayloadEntriesRemoved'] \n",
    "        del df['InstanceToSchemaLinksAdded'] \n",
    "        del df['InstanceToSchemaLinksRemoved'] \n",
    "        del df['TotalNumberOfChangedPayloadElements (real PE_mod)']\n",
    "        del df['TotalNumberOfSchemaElementsWritten']\n",
    "\n",
    "        del df['TotalNumberOfSchemaElementsDeleted']\n",
    "        df = df.set_index('Iteration')\n",
    "        \n",
    "        df.columns = ['SE-ADD', 'SE-DEL','SE-MOD', 'SE-unchanged']\n",
    "        df['instances'] = df2['Imprint links']\n",
    "        df['stability-ratio'] = df['SE-unchanged'] / df['instances']\n",
    "        df['add_del-ratio'] = (df['instances'] - df['SE-MOD'] - df['SE-unchanged'] ) / df['instances']\n",
    "\n",
    "        \n",
    "        change_frame.loc[collection+':min'] = df.min()\n",
    "        change_frame.loc[collection+':max'] = df.max()\n",
    "        change_frame.loc[collection+':mean'] = df.mean()\n",
    "        change_frame.loc[collection+':std'] = df.std()\n",
    "        \n",
    "\n",
    "change_frame.to_csv(out_dir + '/' + appname + '-stats-changes.csv', sep=',', encoding='utf-8')\n",
    "display(change_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_frame = pd.DataFrame(columns=['Load Graph', 'Parse Graph', 'Partition Graph', 'Schema Computation',\n",
    "       'Updates', 'Total', 'Batch', 'Seepdup'])\n",
    "\n",
    "for collection in collections:\n",
    "    with open(os.path.join(data_dir, appname + '_'+collection+suffix+'-performance.csv'), 'r') as f:\n",
    "        df = pd.read_csv(f, sep=',')\n",
    "        df.head()\n",
    "\n",
    "        df = df.set_index('Iteration')\n",
    "        print(collection)\n",
    "#         del df['Load Graph']\n",
    "#         del df['Parse Graph']\n",
    "#         del df['Partition Graph']\n",
    "#         del df['Schema Computation']\n",
    "#         del df['Updates']\n",
    "#        df.columns = [collection + '-Inc', collection + '-Batch']    \n",
    "        convert = lambda x: x / 1000 / 60\n",
    "        df = df.applymap(convert)  \n",
    "        df['Seepdup'] = df.Batch / df.Total\n",
    "        display(df)\n",
    "        performance_frame.loc[collection+':min'] = df.min()\n",
    "        performance_frame.loc[collection+':max'] = df.max()\n",
    "        performance_frame.loc[collection+':mean'] = df.mean()\n",
    "        performance_frame.loc[collection+':std'] = df.std()\n",
    "display(performance_frame)\n",
    "performance_frame.to_csv(out_dir + '/' + appname + '-stats-performance.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "    \n",
    "    \n",
    "# df = pd.concat(frames, sort=False)\n",
    "# df.columns = ['SchemEX:Incr', 'SchemEX:Batch', \n",
    "#              'AttrColl:Incr', 'AttrColl:Batch',\n",
    "#              'TypeColl:Incr', 'TypeColl:Batch']\n",
    "# print(df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = join(data_dir, data_size_dir)\n",
    "onlyfiles = [f for f in listdir(curr_dir) if isfile(join(curr_dir, f)) and f.endswith(\".txt\")]\n",
    "onlyfiles = sorted(onlyfiles)\n",
    "\n",
    "edgecounts = {}\n",
    "tmp = 0\n",
    "for filename in onlyfiles:\n",
    "    # first line: number of types in dataset (not unique)\n",
    "    # second line: number of edges in dataset (not unique)\n",
    "    f = open(join(curr_dir,filename), 'r')\n",
    "    content = f.read().split('\\n')\n",
    "    if len(content) > 2:\n",
    "        edges = int(content[1])\n",
    "    else:\n",
    "        edges = int(content[0])\n",
    "    if 'iteration' in filename:\n",
    "        iteration = filename.replace('iteration', '')\n",
    "        iteration = iteration.replace('-', '')\n",
    "        iteration = iteration.replace('.txt', '')\n",
    "        iteration = iteration.replace('.gz', '')\n",
    "        iteration = iteration.replace('.nq', '')\n",
    "        iteration = iteration.replace('.nt', '')\n",
    "               \n",
    "        edgecounts[int(iteration)] = edges\n",
    "    else:\n",
    "        edgecounts[tmp] = edges\n",
    "        tmp = tmp + 1\n",
    "        \n",
    "edgeframe = pd.DataFrame.from_dict(edgecounts, orient='index')\n",
    "edgeframe = pd.DataFrame.sort_index(edgeframe)\n",
    "#display(edgeframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_frame = pd.DataFrame(columns= ['SE links', 'Imprint links', 'Schema Elements (SE)',\n",
    "       'Schema Relations (SR)', 'summarization-ratio'])\n",
    "\n",
    "for collection in collections:\n",
    "    with open(os.path.join(data_dir, appname + '_'+collection+suffix+'-update-time-and-space.csv'), 'r') as f:\n",
    "        df = pd.read_csv(f, sep=',')\n",
    "        df.head()\n",
    "\n",
    "        del df['SecondaryIndex Read time (ms)']\n",
    "        del df['SecondaryIndex Write time (ms)']\n",
    "        del df['SecondaryIndex Del time (ms)']\n",
    "        #--->\n",
    "        del df['SecondaryIndex Total time (ms)']\n",
    "        #<----\n",
    "        # number of primary vertices\n",
    "        #del df['SE links']\n",
    "#         if not size_frame.empty:\n",
    "#             del df['Imprint links']\n",
    "        del df['Checksum links']\n",
    "        #del df['Schema Elements (SE)']\n",
    "        #del df['Schema Relations (SR)']\n",
    "\n",
    "        del df['SG Read time (ms)']\n",
    "        del df['SG Write time (ms)']\n",
    "        del df['SG Del time (ms)']\n",
    "\n",
    "        del df['Sec. Index Size (bytes)']\n",
    "        del df['Index Size (bytes)']\n",
    "        del df['Graph Size (bytes)']\n",
    "        #print(df)\n",
    "        df = df.set_index('Iteration')\n",
    "        print(df.columns)\n",
    "        size_frame.loc[collection+':min'] = df.min()\n",
    "        size_frame.loc[collection+':max'] = df.max()\n",
    "        size_frame.loc[collection+':mean'] = df.mean()\n",
    "        size_frame.loc[collection+':std'] = df.std()\n",
    "        size_frame['summarization-ratio'] = size_frame['Imprint links'] / size_frame['SE links']\n",
    "        \n",
    "\n",
    "\n",
    "display(size_frame)\n",
    "size_frame.to_csv(out_dir + '/' + appname + '-stats-size.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_frame = pd.DataFrame()\n",
    "data_dir = \"test\"\n",
    "collections = ['schemex', 'type', 'attribute']#,] #, 'type', 'attribute'] # 'type' 'attribute' \n",
    "#collections = ['schemex']\n",
    "\n",
    "appnames = [\"LUBM\",\"BSBM\",\"dyldo_y2019_core\"] # \"dyldo_y2019_full\", #\"LUBM\", \"BSBM\"\n",
    "#appnames = [\"BSBM\"]#, \"LUBM\", \"BSBM\"] # \"dyldo_y2019_full\", #\"LUBM\", \"BSBM\"\n",
    "\n",
    "suffix = ''\n",
    "for appname in appnames:\n",
    "    for collection in collections:\n",
    "        with open(os.path.join(data_dir, appname + '_'+collection+suffix+'-performance.csv'), 'r') as f:\n",
    "            \n",
    "            df = pd.read_csv(f, sep=',')\n",
    "            display(df.head())\n",
    "            temp_frame = pd.DataFrame()\n",
    "            #temp_frame['Iteration'] = df['Iteration']\n",
    "            #df = df.set_index('Iteration')\n",
    "\n",
    "            #temp_frame = temp_frame.set_index('Iteration')\n",
    "\n",
    "            temp_frame['schema-computation'] = df['Schema Computation']\n",
    "            temp_frame['performance'] = df.Total\n",
    "            #temp_frame['speed-up'] = df.Batch / df.Total\n",
    "            temp_frame['updates'] = df['Updates']\n",
    "            f2 = open(os.path.join(data_dir, appname + '_'+collection+suffix+'-changes.csv'), 'r')\n",
    "            df2 = pd.read_csv(f2, sep=',')\n",
    "            #df2 = df2.set_index('Iteration')\n",
    "\n",
    "            display(df2)\n",
    "\n",
    "\n",
    "            f3 = open(os.path.join(data_dir, appname + '_'+collection+suffix+'-update-time-and-space.csv'), 'r')\n",
    "            df3 = pd.read_csv(f3, sep=',')\n",
    "            df3 = df3.set_index('Iteration')\n",
    "            temp_frame['size'] = df3['Imprint links']\n",
    "            df['instances'] = df3['Imprint links']\n",
    "            df['SE-unchanged'] = df2['InstanceNotChanged (PE_mod)']\n",
    "\n",
    "            #display(df)\n",
    "            #display(degree_frame['max_degree'])\n",
    "            temp_frame['max-degree'] = degree_frame['max_degree']\n",
    "            temp_frame['avg-degree'] = degree_frame['avg_degree']\n",
    "\n",
    "            temp_frame['change-ratio'] = ((df3['Imprint links'] - df2['InstanceNotChanged (PE_mod)']) / df3['Imprint links'])\n",
    "            temp_frame['update-ratio'] = (df2['NewlyObservedSchema (SE_new)'] + df2['DeletedSchemaStructures (SE_del)']) / df3['Schema Elements (SE)']\n",
    "            temp_frame['total-changes'] = (df3['Imprint links'] - df2['InstanceNotChanged (PE_mod)'])\n",
    "            temp_frame['total-updates'] = (df2['NewlyObservedSchema (SE_new)'] + df2['DeletedSchemaStructures (SE_del)'])\n",
    "            temp_frame['summary-size'] = df3['Schema Elements (SE)']\n",
    "#             temp_frame['change-ratio'] = 1 - (df2['InstanceNotChanged (PE_mod)'] / df3['Imprint links'].shift(1))\n",
    "            temp_frame['summarization-ratio'] =  df3['Imprint links']/ df3['SE links'].shift(1)\n",
    "#             temp_frame['summary-mod-ratio'] = (df2['TotalNumberOfSchemaElementsWritten'] + df2['TotalNumberOfSchemaElementsDeleted']) /(1 * df3['Schema Elements (SE)'].shift(1)\n",
    "#)# \n",
    "            # test new summary graphs\n",
    "            temp_frame['summary-adds'] = df2['NewlyObservedSchema (SE_new)']\n",
    "            \n",
    "            temp_frame['add-vs-updates'] = df2['NewlyObservedSchema (SE_new)'] / df2['TotalNumberOfSchemaElementsWritten']\n",
    "            temp_frame['del-vs-update'] = df2['DeletedSchemaStructures (SE_del)'] / df2['TotalNumberOfSchemaElementsDeleted']\n",
    "            \n",
    "            #display(temp_frame)\n",
    "            if correlation_frame.empty:\n",
    "                correlation_frame = temp_frame.iloc[1:]\n",
    "            else:\n",
    "                correlation_frame = correlation_frame.append(temp_frame.iloc[1:])\n",
    "\n",
    "                \n",
    "\n",
    "correlation_frame['combined'] = correlation_frame['size'] * correlation_frame['summarization-ratio']            \n",
    "\n",
    "#display(degree_frame)\n",
    "#correlation_frame['ssss'] = degree_frame['avg_degree']\n",
    "correlation_frame['add-time'] = correlation_frame['summary-adds'] / correlation_frame['updates']            \n",
    "\n",
    "display(correlation_frame)\n",
    "\n",
    "print(correlation_frame['add-time'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ms per addition')\n",
    "print(correlation_frame['add-time'].mean())\n",
    "print(correlation_frame['add-time'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = ['size'] #'change-ratio'\n",
    "y_value = ['schema-computation']\n",
    "X = correlation_frame[x_values].values # , 'change-ratio'\n",
    "y = correlation_frame[y_value].values\n",
    "\n",
    "#display(X)\n",
    "\n",
    "lin = LinearRegression() \n",
    "a = [v[0] for v in X]\n",
    "#b = [v[1] for v in X]\n",
    "#display(a)\n",
    "lin.fit(X, y) \n",
    "display(lin.coef_)\n",
    "# Visualising the Linear Regression results \n",
    "plt.scatter(a, y, color = 'blue') \n",
    "#plt.scatter(b, y, color = 'green') \n",
    "\n",
    "plt.plot(X, lin.predict(X), color = 'red') \n",
    "plt.title('Linear Regression') \n",
    "plt.xlabel(x_values) \n",
    "plt.ylabel(y_value) \n",
    "\n",
    "plt.show() \n",
    "lin.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total berechnung hängt vom Datensatz ab, nicht nur von der Größe (param = 1)\n",
    "# Change ratio nicht unbedingt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = correlation_frame.iloc[:, 4:5].values \n",
    "y = correlation_frame.iloc[:, 0].values \n",
    "lin = LinearRegression() \n",
    "\n",
    "lin.fit(X, y) \n",
    "# Visualising the Linear Regression results \n",
    "plt.scatter(X, y, color = 'blue') \n",
    "\n",
    "plt.plot(X, lin.predict(X), color = 'red') \n",
    "plt.title('Linear Regression') \n",
    "plt.xlabel(correlation_frame.columns[4]) \n",
    "plt.ylabel(correlation_frame.columns[0]) \n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = correlation_frame.iloc[:, 3:4].values \n",
    "y = correlation_frame.iloc[:, 0].values \n",
    "lin = LinearRegression() \n",
    "\n",
    "lin.fit(X, y) \n",
    "# Visualising the Linear Regression results \n",
    "plt.scatter(X, y, color = 'blue') \n",
    "\n",
    "plt.plot(X, lin.predict(X), color = 'red') \n",
    "plt.title('Linear Regression') \n",
    "plt.xlabel('Combined-factor ratio') \n",
    "plt.ylabel('Speed-up') \n",
    "\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
